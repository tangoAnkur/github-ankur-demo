{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\spark-2.4.5-bin-hadoop2.7\\spark-2.4.5-bin-hadoop2.7')\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark import SQLContext\n",
    "sc = SparkContext(master='local',appName=\"My Demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql.read.json(\"D:/NYSE/output_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df =sql.read.json('D:/NYSE/data-master/retail_db_json/order_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = sc.textFile(\"D:/NYSE/data-master/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItems = sc.textFile(\"D:/NYSE/data-master/retail_db/order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = sc.textFile(\"D:/NYSE/data-master/retail_db/products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map And FlatMap\n",
    "<code>\n",
    "#status =order.map((lambda status:(status.split(\",\")[3],1)))\n",
    "linesList = [\"Hey Baby\",\"How are you\",\"How is your mind\",\"Do you have a good time\"]\n",
    "lines = sc.parallelize(linesList)\n",
    "wdcnt = lines.flatMap(lambda a:a.split(\" \"))\n",
    "from operator import add\n",
    "result = wdcnt.map(lambda a:(a,1))\n",
    "result.reduceByKey(lambda a,b:a+b).collect()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter in PySPark\n",
    "<code>\n",
    "#Boolean ondition and Filter Use in Data\n",
    "completedOrder = order.filter(lambda a:a.split(\",\")[3]==\"COMPLETE\")\n",
    "completedOrder = order\\\n",
    ".filter(lambda a:(a.split(\",\")[3] in [\"COMPLETE\" ,\"CLOSED\"] and a.split(',')[1].split(\" \")[0][0:7]==\"2013-07\"))\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join in PySpark\n",
    "<code>\n",
    "orderInPair = order.map(lambda a:(int(a.split(\",\")[0]),a.split(\",\")[3]))\n",
    "itemsInPair= orderItems.map(lambda a:(int(a.split(\",\")[1]),float(a.split(\",\")[4])))\n",
    "rightDF.filter(lambda o:o[1][0]==None).take(10)\n",
    "joinedDf.filter(lambda o:o[1][1]==None).take(10)\n",
    "fullouterJoin = orderInPair.fullOuterJoin(itemsInPair)\n",
    "joinedDfinner = orderInPair.join(itemsInPair)\n",
    "joinedDf = orderInPair.leftOuterJoin(itemsInPair)\n",
    "rightDF = itemsInPair.rightOuterJoin(orderInPair)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation in PySpark\n",
    "<code>\n",
    "from operator import add\n",
    "secStage = orderItems.map(lambda a:(int(a.split(\",\")[1]),float(a.split(\",\")[4])))\n",
    "minOde =orderItems.filter(lambda a:int(a.split(\",\")[1])==2)\n",
    "result = minOde.\\\n",
    "reduce(lambda a,b:\n",
    "       a if (float(a.split(\",\")[4]) < float(b.split(\",\")[4])) else b\n",
    "            )\n",
    "orderStatus =order.\\\n",
    "map(lambda o: (o.split(\",\")[3],1))\n",
    "countByStatus =orderStatus.countByKey()# Not Suitable because after that again we need to convert into RDD\n",
    "\n",
    "\n",
    "### GroupByKey()- Two use case 1) sum of revenue 2) sort in Descending order\n",
    "orMap=orderItems.map(lambda oi:(int(oi.split(\",\")[1]),float(oi.split(\",\")[4])))\n",
    "df =orMap.groupByKey()\n",
    "df.map(lambda oi:(oi[0],round(sum(oi[1]),2))).take(5)\n",
    "\n",
    "orMapw =orderItems.map(lambda oi:(int(oi.split(\",\")[1]),oi))\n",
    "ff =orMapw.groupByKey()\n",
    "fd =ff.flatMap(lambda oo : (sorted(oo[1],key = lambda k:float(k.split(\",\")[4]),reverse=True)))\n",
    "\n",
    "#RduceByKey\n",
    "rbk = orderItems.map(lambda oi: (int(oi.split(\",\")[1]),oi))\n",
    "from operator import add\n",
    "rbk.reduceByKey(add)\n",
    "rbk.reduceByKey(lambda a,b: a if(float(a.split(\",\")[4])<float(a.split(\",\")[4])) else b)\n",
    "\n",
    "#### AggregateByKey(2 lambda function and scopeValue)- Revenue and count(Combiner and reducer logic different)\n",
    "revMap =orderItems.map(lambda oi:(int(oi.split(\",\")[1]),float(oi.split(\",\")[4])))\n",
    "revenueCount = revMap.aggregateByKey((0.0,0),lambda a,b:(a[0]+b,a[1]+1),\n",
    "                                     lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
    "</code>\n",
    "#####  Never use GroupBy unless ReduceByKey or AggregateByKey is enough for a problem to solve \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting in PySPark\n",
    "<code>\n",
    "df =product.filter(lambda a:a.split(\",\")[4] != \"\")\\\n",
    "   .map(lambda a:(float(a.split(\",\")[4]),a))\n",
    "resultHelper =df.sortByKey(True)\n",
    "result = resultHelper.map(lambda a:a[1])\n",
    "\n",
    "dfn =product.filter(lambda p:p.split(\",\")[4] != \"\")\\\n",
    "   .map(lambda a:((int(a.split(\",\")[1]),float(a.split(\",\")[4])),a))\n",
    "#### Tricks \n",
    "dfv =product.filter(lambda p:p.split(\",\")[4] != \"\")\\\n",
    "   .map(lambda a:((int(a.split(\",\")[1]),-float(a.split(\",\")[4])),a))\n",
    "resultHelp =dfv.sortByKey()# sort by one ascending another decnding by appending \"-\" before the data in DESC\n",
    "result = resultHelp.map(lambda a:a[1])\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking and its power in Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "new =product.filter(lambda p:p.split(\",\")[4] != \"\")\\\n",
    "   .map(lambda a:(float(a.split(\",\")[4]),a))\n",
    "   \n",
    "sortByPrice = new.sortByKey(False)\n",
    "sortByPrice.map(lambda a:a[1])\n",
    "productFilter.top(5,key=lambda a:float(a.split(',')[4]))#top function\n",
    "productFilter.takeOrdered(5,key=lambda a:-float(a.split(',')[4]))#takeOrdered\n",
    "df =productFilter.map(lambda a: (int(a.split(\",\")[1]),a))\n",
    "l=df.groupByKey()\n",
    "result =l.flatMap(lambda a:sorted(a[1],key=lambda k: float(k.split(\",\")[4]),reverse=True)[:3])\n",
    "\n",
    "df =productFilter.map(lambda a: (int(a.split(\",\")[1]),a))\n",
    "l=df.groupByKey()\n",
    "newSOrt =sorted(filterd[1], key=lambda a:float(a.split(\",\")[4]),reverse=True)\n",
    "l_Map=map(lambda a:round(float(a.split(\",\")[4]),2),newSOrt)\n",
    "topNPrices =sorted(set(l_Map),reverse=True)[:3]\n",
    "import itertools as it\n",
    "topNPricedProduct =it.takewhile(lambda p:float(p.split(\",\")[4]) in topNPrices,newSOrt)\n",
    "<code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy with complecated logic \n",
    "<code>\n",
    "productFilter=product.filter(lambda a:a.split(\",\")[4] != \"\")\n",
    "df =productFilter.map(lambda a: (int(a.split(\",\")[1]),a))\n",
    "groBy =df.groupByKey()\n",
    "filtr =groBy.filter(lambda a:a[0]==59)\n",
    "def getTopNPriceswithDUp(productOnCategoryId, topN):\n",
    "    newSOrt =sorted(productOnCategoryId[1],\n",
    "                    key=lambda a:float(a.split(\",\")[4])\n",
    "                    ,reverse=True)\n",
    "    l_Map=map(lambda a:round(float(a.split(\",\")[4]),2),newSOrt)\n",
    "    topNPrices =sorted(set(l_Map),reverse=True)[:topN]\n",
    "    print(topNPrices)\n",
    "    import itertools as it\n",
    "    return it.takewhile(lambda p:\n",
    "                        float(p.split(\",\")[4]) in topNPrices,\n",
    "                        newSOrt\n",
    "                        )\n",
    "topNPriceProduct = groBy.flatMap(lambda p:getTopNPriceswithDUp(p,3))#All duplicate + first 3 top rate\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Operation in PySpark RDD for Transformation\n",
    "<code>\n",
    "ord201312 = order\\\n",
    ".filter(lambda a: a.split(\",\")[1][:7]=='2013-12').\\\n",
    "map(lambda a:(int(a.split(\",\")[0]),a))\n",
    "ord201401 = order.filter(lambda a: a.split(\",\")[1][:7]=='2014-01')\\\n",
    "     .map(lambda a: (int(a.split(\",\")[0]), a))\n",
    "          \n",
    "orderItemsMap = orderItems.map(lambda a:(int(a.split(\",\")[1]),a)) \n",
    "\n",
    "ordersJoin201312 = ord201312.join(orderItemsMap)\n",
    "ordersJoin201401 = ord201401.join(orderItemsMap)\n",
    "\n",
    "df =ordersJoin201312.map(lambda a: a[1][1])\n",
    "ddf = ordersJoin201401.map(lambda a: a[1][1])\n",
    "\n",
    "product201312 = df.map(lambda p:int(p.split(\",\")[2]))\n",
    "product201401 = ddf.map(lambda p:int(p.split(\",\")[2]))\n",
    "product201312.union(product201401).distinct()\n",
    "commonProducts =product201312.intersection(product201401)#automatically distinct is applied\n",
    "\n",
    "onlyIn201312 =product201312.subtract(product201401).distinct()\n",
    "onlyIn201401 =product201401.subtract(product201312).distinct()\n",
    "\n",
    "onlyInOneMonth =onlyIn201312.union(onlyIn201401)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Saving\n",
    "<code>\n",
    "revHelper =orderItems.map(lambda a : (int(a.split(\",\")[1]),float(a.split(\",\")[4])))\n",
    "\n",
    "revNue = revHelper.reduceByKey(lambda a,b:round(a+b,2))\n",
    "resultFinal =revNue.map(lambda t:str(t[0])+\"\\t\"+str(t[1]))\n",
    "###  ticks\n",
    "##codecs in /etc/hadoop/conf/core-site.xml for Compression\n",
    "resultFinal.saveAsTextFile(\"D:/NYSE/output_file\",compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\")\n",
    "dfTest = sc.textFile(\"D:/NYSE/output_file\")# Read Compress Data with supported algorithm will able to read the data automatically\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need data to be save as Data Frame to save it as ORC,Parquet,JSON,avro(databricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Save after transformationin PySPark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "revHelper =orderItems.map(lambda a : (int(a.split(\",\")[1]),float(a.split(\",\")[4])))\n",
    "revNue = revHelper.reduceByKey(lambda a,b:round(a+b,2))\n",
    "dfResult =revNue.toDF(schema=[\"order_id\",\"order_revenue\"])\n",
    "dfResult.write.save(\"D:/NYSE/output_file\",\"json\",\"snappy\")\n",
    "##### Only for Text file only need to care for delimiter for other no needed\n",
    "#### dfResult.write.csv(\"D:/NYSE/output_file\")    \n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
